{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (long short-term memory) based Recurrent NeuralNet for share price forecasting\n",
    "###### Abdulla Al Blooshi\n",
    "----------------\n",
    "- As an overview, the idea behind this model architecture is to assign weights to selected features; in this case the **open** and the **highest** price the stock reached for a given day were selected. These learned weights represents the model's view on the importance of said features from recent and previous time blocks and how they affect the price in the coming day(s).\n",
    "- This model will be trained on ADNOC's stock price history and the weights it learns will be saved and used via a transfer learning approach to be tested on and predict Borouge's future stock price.\n",
    "- This was done due to the lack of training data for Borouge's stock price as it (relatively) recently IPO'd\n",
    "<br>\n",
    "<br>\n",
    "> *This is by no means financial advice as I am not a financial expert, and all the data used here is publicly available.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow.keras as keras \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    LSTM\n",
    ")\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting and normalizing the data:\n",
    "   - It is easier for the model to work with numbers that are closer together; namely in the range of (0,1) for our case\n",
    "   - The `MinMaxScaler()` transformation is given by:\n",
    "      \n",
    "        X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))<br>\n",
    "        X_scaled = X_std * (max - min) + min <br>\n",
    "        \n",
    "        *where min, max = feature_range.*\n",
    "        > taken straight from the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "[[0.70333333]\n",
      " [0.70333333]\n",
      " [0.69333333]\n",
      " [0.69      ]\n",
      " [0.68666667]\n",
      " [0.69333333]\n",
      " [0.69      ]\n",
      " [0.68666667]\n",
      " [0.70666667]\n",
      " [0.70333333]]\n",
      "Validation set\n",
      "[[0.8       ]\n",
      " [0.8       ]\n",
      " [0.9       ]\n",
      " [0.86666667]\n",
      " [0.9       ]\n",
      " [0.73333333]\n",
      " [0.7       ]\n",
      " [0.93333333]\n",
      " [0.83333333]\n",
      " [0.8       ]]\n",
      "Test set\n",
      "[[2.64]\n",
      " [2.61]\n",
      " [2.62]\n",
      " [2.62]\n",
      " [2.65]\n",
      " [2.62]\n",
      " [2.62]\n",
      " [2.6 ]\n",
      " [2.6 ]\n",
      " [2.62]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1023, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsplit_dat = pd.read_csv('./data/ADNOCDIST_Historical_Data.csv', index_col='Date')\n",
    "valid_dat = pd.read_csv('./data/ADNOC_Valid.csv', index_col='Date')\n",
    "\n",
    "\n",
    "total_set = unsplit_dat[['Open']].values\n",
    "\n",
    "# *Shuffle set to false for smaller dataset to avoid sampling bias, default behavior is shuffle first then split\n",
    "training_set, test_set = train_test_split(total_set, test_size=0.02, random_state=44, shuffle=False)\n",
    "\n",
    "valid_set = valid_dat[['Open']].values\n",
    "\n",
    "normalizer = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "scaled_train_set = normalizer.fit_transform(training_set)\n",
    "scaled_valid_set = normalizer.fit_transform(valid_set)\n",
    "# scaled_test_set = normalizer.fit_transform(test_set)\n",
    "# scaled_total_set = normalizer.fit_transform(total_set)\n",
    "\n",
    "print(f\"Training set\\n{scaled_train_set[:10]}\\nValidation set\\n{scaled_valid_set[:10]}\\nTest set\\n{test_set[:10]}\")\n",
    "training_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTMs distinguishing features and other important notes:\n",
    "- As per the [paper](http://www.bioinf.jku.at/publications/older/2604.pdf) first proposing this architecture by Hochreiter et al. Constant error carousels (CEC) are the central features of LSTMs. Controlling (deciding) the backward propagation of errors through the network.\n",
    "    - These CEC's are then extended to form what is referred to as a memory cell; the extension adds multiplicative input and output gates. These gates control the contents with in a cell from being propagated and control the cell from activating other units respectively.\n",
    "- RNNs are able to use recently seen previous information and cannot do so with information with larger time lags between them, this is where the LSTM architecture comes into play.\n",
    "- This is another one of the distinguishing features of using the LSTM architecture, its ability to 'remember' or erase parts of previously seen data in a window (or timestep).\n",
    "- By creating a window our training data will be turned into an array of arrays divided into chunks of N, where N would be the size of our timestep/window.\n",
    "    - for example having N be 60 would allow our model to use the previous sixty days of data to make the prediction for the 61st.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(963, 60) (963,)\n"
     ]
    }
   ],
   "source": [
    "# Bismillah\n",
    "#TODO: y_train should predict close prices?\n",
    "#TODO: Consider using sklearn's standardScaler()\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "X_valid=[]\n",
    "y_valid=[]\n",
    "\n",
    "for i in range(60,scaled_train_set.shape[0]):\n",
    "    X_train.append(scaled_train_set[i-60:i,0])\n",
    "    y_train.append(scaled_train_set[i,0])\n",
    "\n",
    "for j in range(60,scaled_valid_set.shape[0]):\n",
    "    X_valid.append(scaled_valid_set[j-60:j,0])\n",
    "    y_valid.append(scaled_valid_set[j,0])\n",
    "\n",
    "# Keras accepts numpy arrays\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "X_valid, y_valid = np.array(X_valid), np.array(y_valid) \n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation Notes:\n",
    "- The model base architecture will be sequential which [\"_groups a linear stack of layers into a tf.keras.Model_\"](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)\n",
    "- Ideally I would've liked to add more dropout layers but I do not have that luxury as data is limited. The dropout layers aids in reducing the amount of overfitting.\n",
    "\n",
    "___However, before we can do that the data must be transformed further into a 3D array with X_train learning examples. In our case it will be of dimension (984,60,1), the 984 comes from the number of samples we have, the 60 is because we grouped our samples into groups of 60, and the 1 is because we want the model to access one feature at each timestep___ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))\n",
    "X_valid = np.reshape(X_valid,(X_valid.shape[0],X_valid.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(963, 60, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 60, 32)            4352      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 60, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 60, 32)            8320      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 60, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 21,025\n",
      "Trainable params: 21,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(units=32, activation='tanh', return_sequences=True, input_shape=(X_train.shape[1],1)))\n",
    "lstm_model.add(Dropout(0.1))\n",
    "lstm_model.add(LSTM(units=32, return_sequences=True))\n",
    "lstm_model.add(Dropout(0.05))\n",
    "lstm_model.add(LSTM(units=32))\n",
    "lstm_model.add(Dropout(0.05))\n",
    "lstm_model.add(Dense(units=1))\n",
    "\n",
    "lstm_model.summary()\n",
    "lstm_model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "31/31 [==============================] - 4s 117ms/step - loss: 0.0031 - val_loss: 0.0282\n",
      "Epoch 2/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0024 - val_loss: 0.0279\n",
      "Epoch 3/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0023 - val_loss: 0.0280\n",
      "Epoch 4/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0024 - val_loss: 0.0272\n",
      "Epoch 5/100\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.0024 - val_loss: 0.0279\n",
      "Epoch 6/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0024 - val_loss: 0.0268\n",
      "Epoch 7/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0021 - val_loss: 0.0312\n",
      "Epoch 8/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0024 - val_loss: 0.0263\n",
      "Epoch 9/100\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.0023 - val_loss: 0.0271\n",
      "Epoch 10/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0023 - val_loss: 0.0266\n",
      "Epoch 11/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0022 - val_loss: 0.0277\n",
      "Epoch 12/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0018 - val_loss: 0.0277\n",
      "Epoch 13/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0019 - val_loss: 0.0268\n",
      "Epoch 14/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0020 - val_loss: 0.0269\n",
      "Epoch 15/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0020 - val_loss: 0.0273\n",
      "Epoch 16/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0017 - val_loss: 0.0271\n",
      "Epoch 17/100\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.0018 - val_loss: 0.0268\n",
      "Epoch 18/100\n",
      "31/31 [==============================] - 2s 63ms/step - loss: 0.0018 - val_loss: 0.0270\n",
      "Epoch 19/100\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.0017 - val_loss: 0.0271\n",
      "Epoch 20/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0020 - val_loss: 0.0301\n",
      "Epoch 21/100\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.0023 - val_loss: 0.0275\n",
      "Epoch 22/100\n",
      "31/31 [==============================] - 2s 63ms/step - loss: 0.0018 - val_loss: 0.0296\n",
      "Epoch 23/100\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 0.0018 - val_loss: 0.0271\n",
      "Epoch 24/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0016 - val_loss: 0.0288\n",
      "Epoch 25/100\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.0018 - val_loss: 0.0270\n",
      "Epoch 26/100\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.0018 - val_loss: 0.0271\n",
      "Epoch 27/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0016 - val_loss: 0.0271\n",
      "Epoch 28/100\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 0.0016 - val_loss: 0.0274\n",
      "Epoch 29/100\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 0.0015 - val_loss: 0.0274\n",
      "Epoch 30/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0014 - val_loss: 0.0276\n",
      "Epoch 31/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0015 - val_loss: 0.0277\n",
      "Epoch 32/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0016 - val_loss: 0.0283\n",
      "Epoch 33/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0016 - val_loss: 0.0274\n",
      "Epoch 34/100\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 0.0014 - val_loss: 0.0273\n",
      "Epoch 35/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0015 - val_loss: 0.0271\n",
      "Epoch 36/100\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 0.0015 - val_loss: 0.0270\n",
      "Epoch 37/100\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 0.0016 - val_loss: 0.0272\n",
      "Epoch 38/100\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 0.0014 - val_loss: 0.0277\n",
      "Epoch 39/100\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 0.0014 - val_loss: 0.0275\n",
      "Epoch 40/100\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 0.0014 - val_loss: 0.0271\n",
      "Epoch 41/100\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 0.0013 - val_loss: 0.0266\n",
      "Epoch 42/100\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.0014 - val_loss: 0.0265\n",
      "Epoch 43/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0014 - val_loss: 0.0270\n",
      "Epoch 44/100\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.0014 - val_loss: 0.0271\n",
      "Epoch 45/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0013 - val_loss: 0.0272\n",
      "Epoch 46/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0016 - val_loss: 0.0277\n",
      "Epoch 47/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0012 - val_loss: 0.0266\n",
      "Epoch 48/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0013 - val_loss: 0.0264\n",
      "Epoch 49/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0013 - val_loss: 0.0268\n",
      "Epoch 50/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0012 - val_loss: 0.0265\n",
      "Epoch 51/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0012 - val_loss: 0.0262\n",
      "Epoch 52/100\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.0013 - val_loss: 0.0256\n",
      "Epoch 53/100\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.0011 - val_loss: 0.0264\n",
      "Epoch 54/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0012 - val_loss: 0.0259\n",
      "Epoch 55/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0012 - val_loss: 0.0261\n",
      "Epoch 56/100\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 0.0012 - val_loss: 0.0264\n",
      "Epoch 57/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0012 - val_loss: 0.0265\n",
      "Epoch 58/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0012 - val_loss: 0.0260\n",
      "Epoch 59/100\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 0.0013 - val_loss: 0.0250\n",
      "Epoch 60/100\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 0.0012 - val_loss: 0.0252\n",
      "Epoch 61/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0012 - val_loss: 0.0251\n",
      "Epoch 62/100\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 0.0011 - val_loss: 0.0248\n",
      "Epoch 63/100\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 9.4442e-04 - val_loss: 0.0245\n",
      "Epoch 64/100\n",
      "31/31 [==============================] - 2s 70ms/step - loss: 9.5424e-04 - val_loss: 0.0251\n",
      "Epoch 65/100\n",
      "31/31 [==============================] - 2s 69ms/step - loss: 0.0012 - val_loss: 0.0241\n",
      "Epoch 66/100\n",
      "31/31 [==============================] - 2s 71ms/step - loss: 0.0011 - val_loss: 0.0244\n",
      "Epoch 67/100\n",
      "31/31 [==============================] - 2s 66ms/step - loss: 0.0010 - val_loss: 0.0248\n",
      "Epoch 68/100\n",
      "31/31 [==============================] - 2s 71ms/step - loss: 0.0012 - val_loss: 0.0243\n",
      "Epoch 69/100\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 0.0013 - val_loss: 0.0237\n",
      "Epoch 70/100\n",
      "31/31 [==============================] - 2s 63ms/step - loss: 0.0011 - val_loss: 0.0235\n",
      "Epoch 71/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0011 - val_loss: 0.0234\n",
      "Epoch 72/100\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 9.8734e-04 - val_loss: 0.0232\n",
      "Epoch 73/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 9.3663e-04 - val_loss: 0.0235\n",
      "Epoch 74/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 8.7041e-04 - val_loss: 0.0236\n",
      "Epoch 75/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 8.8486e-04 - val_loss: 0.0234\n",
      "Epoch 76/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0012 - val_loss: 0.0226\n",
      "Epoch 77/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 9.9524e-04 - val_loss: 0.0228\n",
      "Epoch 78/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 9.0687e-04 - val_loss: 0.0228\n",
      "Epoch 79/100\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.0012 - val_loss: 0.0225\n",
      "Epoch 80/100\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 9.2892e-04 - val_loss: 0.0218\n",
      "Epoch 81/100\n",
      "31/31 [==============================] - 2s 63ms/step - loss: 9.3834e-04 - val_loss: 0.0227\n",
      "Epoch 82/100\n",
      "31/31 [==============================] - 2s 81ms/step - loss: 8.8277e-04 - val_loss: 0.0224\n",
      "Epoch 83/100\n",
      "31/31 [==============================] - 2s 66ms/step - loss: 0.0011 - val_loss: 0.0222\n",
      "Epoch 84/100\n",
      "31/31 [==============================] - 2s 63ms/step - loss: 9.0238e-04 - val_loss: 0.0216\n",
      "Epoch 85/100\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 9.0688e-04 - val_loss: 0.0215\n",
      "Epoch 86/100\n",
      "31/31 [==============================] - 2s 65ms/step - loss: 0.0012 - val_loss: 0.0215\n",
      "Epoch 87/100\n",
      "31/31 [==============================] - 2s 73ms/step - loss: 8.6804e-04 - val_loss: 0.0213\n",
      "Epoch 88/100\n",
      "31/31 [==============================] - 2s 65ms/step - loss: 8.6496e-04 - val_loss: 0.0210\n",
      "Epoch 89/100\n",
      "31/31 [==============================] - 2s 74ms/step - loss: 9.3026e-04 - val_loss: 0.0211\n",
      "Epoch 90/100\n",
      "31/31 [==============================] - 4s 135ms/step - loss: 8.8745e-04 - val_loss: 0.0215\n",
      "Epoch 91/100\n",
      "31/31 [==============================] - 4s 135ms/step - loss: 8.8544e-04 - val_loss: 0.0207\n",
      "Epoch 92/100\n",
      "31/31 [==============================] - 4s 132ms/step - loss: 8.2809e-04 - val_loss: 0.0203\n",
      "Epoch 93/100\n",
      "31/31 [==============================] - 4s 141ms/step - loss: 8.5404e-04 - val_loss: 0.0213\n",
      "Epoch 94/100\n",
      "31/31 [==============================] - 3s 82ms/step - loss: 8.5877e-04 - val_loss: 0.0200\n",
      "Epoch 95/100\n",
      "31/31 [==============================] - 2s 63ms/step - loss: 8.2938e-04 - val_loss: 0.0201\n",
      "Epoch 96/100\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 8.3338e-04 - val_loss: 0.0208\n",
      "Epoch 97/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0010 - val_loss: 0.0199\n",
      "Epoch 98/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 7.4963e-04 - val_loss: 0.0199\n",
      "Epoch 99/100\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 8.2628e-04 - val_loss: 0.0202\n",
      "Epoch 100/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0012 - val_loss: 0.0193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2c144698460>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(X_train, y_train, epochs=100,batch_size=32,validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model on unseen data\n",
    "\n",
    "- After training the validation loss is reasonably larger than loss on the training set, hinting towards the possibility of underfitting \n",
    "- Possible solutions are:\n",
    "    - Parameter increase\n",
    "    - Additional layers of\n",
    "    - Increased training set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense\t Dropout\t LSTM\t MinMaxScaler\t SGD\t Sequential\t X_test\t X_train\t X_valid\t \n",
      "e\t i\t j\t keras\t lstm_model\t model_prediction\t normalizer\t np\t os\t \n",
      "pd\t scaled_train_set\t scaled_valid_set\t sys\t test_set\t tests\t total_set\t train_test_split\t training_set\t \n",
      "unsplit_dat\t valid_dat\t valid_set\t y_train\t y_valid\t \n"
     ]
    }
   ],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.509372 ],\n",
       "       [3.5098052],\n",
       "       [3.5101452],\n",
       "       [3.5103612],\n",
       "       [3.5105457],\n",
       "       [3.5106266],\n",
       "       [3.5105953],\n",
       "       [3.5104556],\n",
       "       [3.5103636],\n",
       "       [3.510312 ],\n",
       "       [3.5103216],\n",
       "       [3.5103247],\n",
       "       [3.5103552],\n",
       "       [3.510565 ],\n",
       "       [3.5107515],\n",
       "       [3.5108151],\n",
       "       [3.5106323],\n",
       "       [3.5100656],\n",
       "       [3.5091448],\n",
       "       [3.5082622],\n",
       "       [3.5077653]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(scaled_total_set)-len(scaled_test_set)-60 = 963 = X_train.shape[0], the remaining being test\n",
    "tests=total_set[len(total_set)-len(test_set)-60:]\n",
    "tests = tests.reshape(-1,1)\n",
    "# tests = tests.reshape(-1,1)\n",
    "tests = normalizer.transform(tests)\n",
    "\n",
    "X_test=[]\n",
    "# Creating the same window and dimensions as before\n",
    "for e in range(60,tests.shape[0]):\n",
    "    X_test.append(tests[e-60:e,0])\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1],1))\n",
    "\n",
    "model_prediction = lstm_model.predict(X_test)\n",
    "model_prediction = normalizer.inverse_transform(model_prediction)\n",
    "model_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"color:orange\">\n",
    "<li>TODO: ExponMovingAvg for comparison</li>\n",
    "<li>TODO: Need to resolve package conflicts with matplotlib, and plot results nicely...</li>\n",
    "<li>TODO: Save model and use transfer learning to apply to Borouge stock</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mainenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "214b7b0af70bfa28e0afdd81120454c4d11168c6ab8419e440f91c87a78b14e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
